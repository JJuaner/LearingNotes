1.分类/回归任务<br>

2.liner regression/logistics regression
>> liner=wx+b / logistics=liner+sigmoid=sigmoid(wx+b) <br>
>> 预测值在连续空间/预测值在0-1连续空间<br>
>> 将∞预测值压缩到0-1之间，适合表示概率问题
>> 

线性回归  均方误差  
逻辑回归/逻辑分类  交叉熵损失刻画真实值与预测值差异 -真实y×log(预测y)/-ylog(y)-(1-y)log(1-y)  
https://blog.csdn.net/tsyccnh/article/details/79163834

3.激活函数
>>1.增加非线性/增加神经网络函数非线性拟合能力 2.生物角度模拟大脑  
>>2+2 公式/图像/适用特征  
>>>sigmoid 将数值映射到0-1之间  
>>>tanh 将数值映射到-1-1之间  
>>>relu

4.矩阵相乘
>>element-wise:对应位置元素相乘<br>
>>matmul:标准矩阵乘法


5.KL散度 https://www.jianshu.com/p/43318a3dc715


6.核函数
>>支持向量机通过某非线性变换 φ( x) ，将输入空间映射到高维特征空间(特征空间的维数可能非常高)。  
>>如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数 K(x, x′) ，它恰好等于在高维空间中这个内积，即K( x, x′) =<φ( x) ⋅φ( x′) > 。  
>>那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′) 直接得到非线性变换的内积，使大大简化了计算。这样的函数 K(x, x′) 称为核函数。  

思路：  
输入空间->非线性映射函数φ->高维特征空间  
在高维空间将非线性问题转换为线性问题     
进一步：
核函数对输入空间向高维空间的一种隐式映射（注意，这也是低维空间到高维空间的一种映射）  
不需要显式的定义特征空间和映射函数，而在输入空间中就可以计算内积，只是用来计算映射到高维空间之后的内积的一种简便方法  
核函数的作用：隐含着一个从低维空间到高维空间的映射，计算样本映射到高维空间之/在高维空间的内积。  
核技巧：巧妙的使用 核函数+线性分类学习方法 解决非线性问题  
>>定理:只要一个对称函数对应的核矩阵半正定，它就能作为核函数->找到自然界可作为核函数的函数。  
>>核函数的使用：依赖经验直觉直接选择核函数，核函数的有效性需要实验验证。  
>>>很多使用者都是盲目地试验各种核函数+扫描其中的参数，选择效果最好的。至于什么样的核函数适用于什么样的问题，大多数人都不懂。  
>>>选择了不同的核函数，意味选择了不同的某种映射。因为我们不知道映射的具体形式，所以我们并不知道什么样的核函数合适。核函数的选择成为算法的“变数”。  


## 降维
why降维(原因/必要性/motivation):高维数据导致**计算量大**和**样本稀疏**的问题,实际上.我们也不需要那么多的特征,部分特征就足够完成任务(特征存在冗余:一方面无用特征.一方面一些特征之间不是独立的,存在线性相关性)
数据维数过大（成千上万）难以处理  
降维：减少维度和特征  
>>1.PCA主成分分析：`分析数据中最主要的成分/特征，保存数据主要的信息`  
`PCA是将数据投影到方差最大的几个相互正交的方向上`，以期待保留最多的样本信息。    
`数据映射到新空间应尽可能分散，即方差越大越好`。样本的方差越大表示样本的多样性越好，在训练模型的时候，我们当然希望数据的差别越大越好。否则即使样本很多但是他们彼此相似或者相同，提供的样本信息将相同，相当于只有很少的样本提供信息是有用的。样本信息不足将导致模型性能不够理想。
>>>PCA和LDA都是讲数据投影到新的相互正交的坐标轴上。只不过在投影的过程中他们使用的约束是不同的，也可以说目标是不同的:`PCA是无监督的把数据投影到方差最大的几个相互正交的方向上`，`LDA有监督的将带有标签的数据投影到低维空间同时满足三个条件`

将带有标签的数据降维，投影到低维空间同时满足三个条件：

尽可能多地保留数据样本的信息（即选择最大的特征是对应的特征向量所代表的的方向）。
寻找使样本尽可能好分的最佳投影方向。
投影后使得同类样本尽可能近，不同类样本尽可能远

## 极大似然估计  
从训练样本中自动获得最优模型参数

## 马尔可夫模型  
### MM
对象：处理一维数据（序列/文本）  
任务：建立序列的概率 
起始状态，状态转移矩阵
### HMM
`隐状态``观测状态O`
隐状态链  
隐状态决定观测状态


### 结构风险最小化
结构风险最小化等价于正则化。结构风险在经验风险的基础上加上表示模型复杂度的正则化项（防止过拟合，选择模型效果和复杂度都好的模型/奥卡姆剃刀原理）


## 朴素贝叶斯
朴素naive：不考虑相互关系，独立性假设

# 贝叶斯决策理论
贝叶斯决策理论中所用的“先验概率”、“后验概率”以及“贝叶斯公式”是什么？
贝叶斯决策理论用来从历史已有数据（贝叶斯公式）做决策  
利用从数据中可观测的**先验概率**，使用**贝叶斯公式**计算不可知的**后验概率**     
贝叶斯最优决策规则：选择后验概率最大的结果（**贝叶斯决策的预测值是选择后验概率最大的结果**）　　
贝叶斯风险:使用贝叶斯决策规则的风险(理论上最小风险,我们无法准确估计需要的概率分布,我们知道存在但求不出来)
https://www.zhihu.com/question/27670909

理解后验概率的计算公式:x为y的概率(后验)=y先验 * y类条件概率   解释是:x为y的概率等于y原本存在的概率乘以给定y条件下x占的比例
(共同的px/数据忽略不计算),通常先验好估计,类条件概率不好估计

## 信息论 
### mutual information
I(X,Y)=P(X,Y)log(P(X,Y)/P(x)P(y)) XY同时出现看得见X同时看得见Y，看不见X也看不见Y
### 交叉熵（cross entropy）
交叉熵是深度学习中常用的概念，一般用来求目标与预测值之间的差距。  
交叉熵本质是信息论中的一个概念    
https://blog.csdn.net/tsyccnh/article/details/79163834

## DFT FFT
DFT离散傅里叶变换  
FFT快速傅里叶变换只是一种快速计算DFT的方法(要求在特点的点)  
两个角度理解FT的作用：1.多项式（系数-FFT->求特定点处值，已知n个特定点处值-IFFT->求系数） 2.时域->频域的变换
