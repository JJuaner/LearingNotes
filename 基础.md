1.分类/回归任务<br>

2.liner regression/logistics regression
>> liner=wx+b / logistics=liner+sigmoid=sigmoid(wx+b) <br>
>> 预测值在连续空间/预测值在0-1连续空间<br>
>> 将∞预测值压缩到0-1之间，适合表示概率问题

3.激活函数
>>1.增加非线性/增加神经网络函数非线性拟合能力 2.生物角度模拟大脑  
>>2+2 公式/图像/适用特征  
>>>sigmoid 将数值映射到0-1之间  
>>>tanh 将数值映射到-1-1之间  
>>>relu

4.矩阵相乘
>>element-wise:对应位置元素相乘<br>
>>matmul:标准矩阵乘法


5.KL散度 https://www.jianshu.com/p/43318a3dc715


6.核函数
>>支持向量机通过某非线性变换 φ( x) ，将输入空间映射到高维特征空间(特征空间的维数可能非常高)。  
>>如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数 K(x, x′) ，它恰好等于在高维空间中这个内积，即K( x, x′) =<φ( x) ⋅φ( x′) > 。  
>>那么支持向量机就不用计算复杂的非线性变换，而由这个函数 K(x, x′) 直接得到非线性变换的内积，使大大简化了计算。这样的函数 K(x, x′) 称为核函数。  

思路：  
输入空间->非线性映射函数φ->高维特征空间  
在高维空间将非线性问题转换为线性问题     
进一步：
核函数对输入空间向高维空间的一种隐式映射（注意，这也是低维空间到高维空间的一种映射）  
不需要显式的定义特征空间和映射函数，而在输入空间中就可以计算内积，只是用来计算映射到高维空间之后的内积的一种简便方法  
核函数的作用：隐含着一个从低维空间到高维空间的映射，计算样本映射到高维空间之/在高维空间的内积。  
核技巧：巧妙的使用 核函数+线性分类学习方法 解决非线性问题  
>>定理:只要一个对称函数对应的核矩阵半正定，它就能作为核函数->找到自然界可作为核函数的函数。  
>>核函数的使用：依赖经验直觉直接选择核函数，核函数的有效性需要实验验证。  
>>>很多使用者都是盲目地试验各种核函数+扫描其中的参数，选择效果最好的。至于什么样的核函数适用于什么样的问题，大多数人都不懂。  
>>>选择了不同的核函数，意味选择了不同的某种映射。因为我们不知道映射的具体形式，所以我们并不知道什么样的核函数合适。核函数的选择成为算法的“变数”。  


7.降维
数据维数过大（成千上万）难以处理  
降维：减少维度和特征  
>>1.PCA主成分分析：分析数据中最主要的成分/特征，保存数据主要的信息  
`PCA是将数据投影到方差最大的几个相互正交的方向上`，以期待保留最多的样本信息。    
`数据映射到新空间应尽可能分散，即方差越大越好`。样本的方差越大表示样本的多样性越好，在训练模型的时候，我们当然希望数据的差别越大越好。否则即使样本很多但是他们彼此相似或者相同，提供的样本信息将相同，相当于只有很少的样本提供信息是有用的。样本信息不足将导致模型性能不够理想。
>>>PCA和LDA都是讲数据投影到新的相互正交的坐标轴上。只不过在投影的过程中他们使用的约束是不同的，也可以说目标是不同的:`PCA是无监督的把数据投影到方差最大的几个相互正交的方向上`，`LDA有监督的将带有标签的数据投影到低维空间同时满足三个条件`

将带有标签的数据降维，投影到低维空间同时满足三个条件：

尽可能多地保留数据样本的信息（即选择最大的特征是对应的特征向量所代表的的方向）。
寻找使样本尽可能好分的最佳投影方向。
投影后使得同类样本尽可能近，不同类样本尽可能远
